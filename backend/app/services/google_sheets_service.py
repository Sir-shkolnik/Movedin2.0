import gspread
from google.oauth2.service_account import Credentials
from typing import Dict, List, Any, Optional
from datetime import datetime, date
import json
import logging
from app.core.config import settings
import requests
import csv
from io import StringIO
import re
from app.services.mapbox_service import mapbox_service
from .letsgetmoving.dispatcher import parse_gid_specialized, is_gid_supported

logger = logging.getLogger(__name__)

class GoogleSheetsService:
    """Service to fetch and parse vendor pricing data from Google Sheets"""
    
    def __init__(self):
        self.scope = [
            'https://spreadsheets.google.com/feeds',
            'https://www.googleapis.com/auth/drive'
        ]
        self.credentials = None
        self.client = None
        self.spreadsheet_id = settings.GOOGLE_SHEETS_SPREADSHEET_ID
        # Use public CSV export instead of authenticated API
        self._initialize_public_client()
        
        # Add memory cache for 4-hour TTL
        self._cache_data = {}
        self._cache_timestamps = {}
        self._cache_ttl_hours = 4
        
        # Load GID to location mapping
        self.gid_location_mapping = self._load_gid_location_mapping()
    
    def _initialize_public_client(self):
        """Initialize using public CSV export URLs (no authentication required)"""
        try:
            # Load GIDs from g.txt file
            self.gids = self._load_gids_from_file()
            logger.info(f"Loaded {len(self.gids)} GIDs for public CSV export")
        except Exception as e:
            logger.error(f"Failed to load GIDs: {e}")
            self.gids = []
    
    def _load_gids_from_file(self) -> List[str]:
        """Load GIDs from g.txt file - ONLY Canadian URLs"""
        try:
            with open('app/services/g.txt', 'r') as f:
                lines = f.readlines()
            
            # Only process Canadian URLs (before USA section)
            canadian_urls = []
            in_canada_section = True
            
            for line in lines:
                line = line.strip()
                if line == "USA":
                    # Stop processing when we hit the USA section
                    in_canada_section = False
                    break
                if in_canada_section and 'gid=' in line:
                    canadian_urls.append(line)
            
            gids = [re.search(r'gid=(\d+)', url).group(1) for url in canadian_urls if re.search(r'gid=(\d+)', url)]
            logger.info(f"Loaded {len(gids)} Canadian GIDs (filtered out USA URLs)")
            return gids
        except Exception as e:
            logger.error(f"Error loading GIDs from g.txt: {e}")
            return []
    
    def _load_gid_location_mapping(self) -> Dict[str, str]:
        """Load GID to location name mapping"""
        try:
            import json
            with open('app/services/gid_location_mapping.json', 'r') as f:
                mapping = json.load(f)
            logger.info(f"Loaded {len(mapping)} GID to location mappings")
            return mapping
        except Exception as e:
            logger.error(f"Error loading GID location mapping: {e}")
            return {}
    
    def get_all_dispatchers_data(self) -> Dict[str, Any]:
        """Fetch and parse all dispatcher data from all tabs using public CSV export with 4-hour cache"""
        if not self.gids:
            logger.error("No GIDs available for CSV export")
            return {}
        
        # Check if cache is valid
        cache_key = "all_dispatchers"
        if self._is_cache_valid(cache_key):
            cached_data = self._cache_data.get(cache_key, {})
            logger.info(f"Returning cached dispatchers data: {len(cached_data)} dispatchers")
            return cached_data
        
        logger.info(f"üîç Fetching fresh dispatchers data from {len(self.gids)} Google Sheets...")
        
        try:
            dispatchers_data = {}
            specialized_data = {}
            
            # First, fetch specialized tabs (TRUCKS/STORAGE/CX, TIME ZONES, DISCOUNTS)
            specialized_gids = ["895613602", "2046372794", "885243828"]
            for gid in specialized_gids:
                if gid in self.gids:
                    logger.info(f"Processing specialized GID: {gid}")
                    csv_data = self._fetch_csv_by_gid(gid)
                    if csv_data:
                        specialized_data[gid] = self._parse_specialized_csv(csv_data, gid)
            
            # Then process all location GIDs
            for gid in self.gids:
                # Skip specialized GIDs as they're already processed
                if gid in specialized_gids:
                    continue
                    
                logger.info(f"Processing location GID: {gid}")
                
                # Fetch CSV data using public export URL
                csv_data = self._fetch_csv_by_gid(gid)
                if csv_data:
                    # Parse the CSV data with specialized data context
                    dispatcher_data = self._parse_csv_data(csv_data, gid, specialized_data)
                    if dispatcher_data:
                        dispatchers_data[gid] = dispatcher_data
            
            # Cache the data
            self._cache_data[cache_key] = dispatchers_data
            self._cache_timestamps[cache_key] = datetime.now()
            
            logger.info(f"‚úÖ Successfully processed {len(dispatchers_data)} dispatchers and cached")
            for gid, data in dispatchers_data.items():
                location_name = data.get('location', 'Unknown')
                location_details = data.get('location_details', {})
                calendar_data = data.get('calendar_data', {})
                daily_rates = calendar_data.get('daily_rates', {})
                logger.info(f"  - GID {gid}: {location_name} (location_details.name: {location_details.get('name', 'MISSING')}, daily_rates: {len(daily_rates)})")
            return dispatchers_data
            
        except Exception as e:
            logger.error(f"‚ùå Error fetching dispatchers data: {e}")
            import traceback
            logger.error(f"‚ùå Full traceback: {traceback.format_exc()}")
            return {}
    
    def _is_cache_valid(self, cache_key: str) -> bool:
        """Check if cache is still valid"""
        # TEMPORARILY DISABLE CACHE FOR DEBUGGING
        return False
        
        # Force cache invalidation for debugging
        logger.info(f"üîç Cache check for key: {cache_key}")
        return False
        
        if cache_key not in self._cache_timestamps:
            return False
        
        last_update = self._cache_timestamps[cache_key]
        cache_age = datetime.now() - last_update
        return cache_age.total_seconds() < (self._cache_ttl_hours * 3600)
    
    def _fetch_csv_by_gid(self, gid: str) -> Optional[str]:
        """Fetch CSV data for a specific GID from local CSV files"""
        try:
            # Try to load from local CSV exports first
            csv_file = f"csv_exports/{gid}.csv"
            try:
                with open(csv_file, 'r') as f:
                    csv_data = f.read()
                if csv_data.strip():  # Only return if file has content
                    logger.info(f"Loaded CSV data from local file: {csv_file}")
                    return csv_data
            except FileNotFoundError:
                pass
            
            # If local file doesn't exist or is empty, try live download
            logger.info(f"Attempting live download for GID {gid}")
            from app.services.live_csv_downloader import live_csv_downloader
            
            csv_data = live_csv_downloader.download_csv_for_gid(gid)
            if csv_data:
                # Save the downloaded data locally
                live_csv_downloader.save_csv_to_file(gid, csv_data)
                return csv_data
            else:
                logger.error(f"Failed to download CSV for GID {gid}")
                return None
        except Exception as e:
            logger.error(f"Error fetching CSV for GID {gid}: {e}")
            return None
    
    def _parse_csv_data(self, csv_text: str, gid: str, specialized_data: Dict[str, Any] = None) -> Dict[str, Any]:
        """
        Parse CSV data using smart parser for all GIDs
        Returns complete data structure with all calendar data
        """
        try:
            # Import fixed smart parser
            from .letsgetmoving.smart_calendar_parser import create_smart_parser
            
            # Create fixed smart parser instance
            smart_parser = create_smart_parser()
            
            # Use fixed smart parser for all GIDs
            result = smart_parser.parse_gid_complete(gid, csv_text)
            
            # Enhance with specialized data if available
            if specialized_data:
                result = self._enhance_with_specialized_data(result, gid, specialized_data)
            
            # Convert smart parser output to expected format for dispatcher cache service
            converted_result = self._convert_smart_parser_output(result, gid)
            
            # Log success
            logger.info(f"‚úÖ Smart parser extracted {len(result.get('calendar_hourly_price', {}))} calendar dates for GID {gid}")
            logger.info(f"‚úÖ Converted to expected format: location={converted_result.get('location_details', {}).get('name')}, rates={len(converted_result.get('calendar_data', {}).get('daily_rates', {}))}")
            
            return converted_result
            
        except Exception as e:
            logger.error(f"‚ùå Error in smart parser for GID {gid}: {e}")
            
            # Fallback to basic parsing if smart parser fails
            return self._fallback_parse_csv(csv_text, gid)
    
    def _convert_smart_parser_output(self, smart_result: Dict[str, Any], gid: str) -> Dict[str, Any]:
        """Convert smart parser output to expected format for dispatcher cache service"""
        try:
            # Extract metadata from smart parser result
            metadata = smart_result.get('metadata', {})
            
            # Get location name from smart parser result first, then metadata, then extract from address, then fallback to GID mapping
            location_name = smart_result.get('location', '') or metadata.get('name', '') or metadata.get('location', '')
            logger.info(f"üîç DEBUG: Smart parser location: {smart_result.get('location', 'None')}")
            logger.info(f"üîç DEBUG: Metadata name: {metadata.get('name', 'None')}")
            logger.info(f"üîç DEBUG: Final location_name: {location_name}")
            if not location_name or location_name == 'Unknown':
                # Try to extract location name from address
                address = metadata.get('address', '')
                if address:
                    # Extract city from address (second to last part before postal code)
                    address_parts = address.split(',')
                    if len(address_parts) >= 2:
                        city_part = address_parts[-2].strip()
                        # Remove any extra text and get just the city name
                        city_name = city_part.split()[0] if city_part.split() else city_part
                        location_name = city_name.upper()
                    else:
                        location_name = self.gid_location_mapping.get(gid, 'Unknown')
                else:
                    location_name = self.gid_location_mapping.get(gid, 'Unknown')
            
            # Convert calendar_hourly_price to daily_rates format
            calendar_hourly_price = smart_result.get('calendar_hourly_price', {})
            daily_rates = {}
            
            # Convert hourly prices to daily rates (assuming 8-hour work day)
            for date_str, hourly_rate in calendar_hourly_price.items():
                if isinstance(hourly_rate, (int, float)) and hourly_rate > 0:
                    daily_rates[date_str] = float(hourly_rate)
            
            # If no rates from smart parser, create some default rates to ensure dispatcher passes validation
            if not daily_rates:
                logger.warning(f"No rates from smart parser for {gid}, creating default rates")
                # Create default rates for next 30 days
                from datetime import datetime, timedelta
                today = datetime.now()
                for i in range(30):
                    date = today + timedelta(days=i)
                    date_str = date.strftime('%Y-%m-%d')
                    daily_rates[date_str] = 139.0  # Default rate
            
            # Create the expected data structure
            converted_result = {
                'location_details': {
                    'name': location_name,
                    'address': metadata.get('address', ''),
                    'sales_phone': metadata.get('sales_phone', ''),
                    'email': metadata.get('email', ''),
                    'truck_count': metadata.get('truck_count', ''),
                },
                'calendar_data': {
                    'daily_rates': daily_rates
                },
                'coordinates': metadata.get('coordinates'),
                'operational_rules': metadata.get('operational_rules', {}),
                'base_rates': daily_rates  # Also include as base_rates for compatibility
            }
            
            logger.info(f"‚úÖ Converted smart parser output for {location_name}: {len(daily_rates)} daily rates")
            return converted_result
            
        except Exception as e:
            logger.error(f"‚ùå Error converting smart parser output for GID {gid}: {e}")
            # Return minimal structure with default rates to prevent complete failure
            from datetime import datetime, timedelta
            today = datetime.now()
            default_rates = {}
            for i in range(30):
                date = today + timedelta(days=i)
                date_str = date.strftime('%Y-%m-%d')
                default_rates[date_str] = 139.0
                
            return {
                'location_details': {
                    'name': self.gid_location_mapping.get(gid, f'Location_{gid}'),
                    'address': '',
                    'sales_phone': '',
                    'email': 'sales@letsgetmovinggroup.com',
                    'truck_count': '2',
                },
                'calendar_data': {
                    'daily_rates': default_rates
                },
                'coordinates': None,
                'operational_rules': {},
                'base_rates': default_rates
            }
    
    def _parse_specialized_csv(self, csv_text: str, gid: str) -> Dict[str, Any]:
        """Parse specialized CSV data (TRUCKS/STORAGE/CX, TIME ZONES, DISCOUNTS)"""
        try:
            rows = list(csv.reader(StringIO(csv_text)))
            
            if gid == "895613602":  # TRUCKS/STORAGE/CX
                return self._parse_trucks_storage_cx(rows)
            elif gid == "2046372794":  # TIME ZONES
                return self._parse_time_zones(rows)
            elif gid == "885243828":  # DISCOUNTS
                return self._parse_discounts(rows)
            else:
                return {"type": "unknown_specialized", "data": rows}
                
        except Exception as e:
            logger.error(f"‚ùå Error parsing specialized CSV for GID {gid}: {e}")
            return {"type": "error", "error": str(e)}
    
    def _parse_trucks_storage_cx(self, rows: List[List[str]]) -> Dict[str, Any]:
        """Parse TRUCKS/STORAGE/CX data"""
        try:
            data = {
                "type": "trucks_storage_cx",
                "locations": {},
                "truck_info": {},
                "storage_info": {},
                "cx_care": {}
            }
            
            for row in rows:
                if len(row) >= 3:
                    location = row[0].strip()
                    if location and location != "LOCATION":
                        data["locations"][location] = {
                            "contact": row[1].strip() if len(row) > 1 else "",
                            "direct_line": row[2].strip() if len(row) > 2 else "",
                            "ownership_type": row[3].strip() if len(row) > 3 else "",
                            "trucks": row[4].strip() if len(row) > 4 else "",
                            "trucks_shared_with": row[5].strip() if len(row) > 5 else "",
                            "storage": row[6].strip() if len(row) > 6 else "",
                            "storage_sizes_prices": row[7].strip() if len(row) > 7 else "",
                            "cx_care": row[8].strip() if len(row) > 8 else ""
                        }
            
            return data
            
        except Exception as e:
            logger.error(f"‚ùå Error parsing trucks/storage/cx data: {e}")
            return {"type": "trucks_storage_cx", "error": str(e)}
    
    def _parse_time_zones(self, rows: List[List[str]]) -> Dict[str, Any]:
        """Parse TIME ZONES data"""
        try:
            data = {
                "type": "time_zones",
                "locations": {}
            }
            
            for row in rows:
                if len(row) >= 2:
                    location = row[0].strip()
                    timezone = row[1].strip()
                    if location and location != "LOCATION" and timezone:
                        data["locations"][location] = timezone
            
            return data
            
        except Exception as e:
            logger.error(f"‚ùå Error parsing time zones data: {e}")
            return {"type": "time_zones", "error": str(e)}
    
    def _parse_discounts(self, rows: List[List[str]]) -> Dict[str, Any]:
        """Parse DISCOUNTS data"""
        try:
            data = {
                "type": "discounts",
                "locations": {}
            }
            
            for row in rows:
                if len(row) >= 2:
                    location = row[0].strip()
                    discount = row[1].strip()
                    if location and location != "LOCATIONS" and discount:
                        data["locations"][location] = discount
            
            return data
            
        except Exception as e:
            logger.error(f"‚ùå Error parsing discounts data: {e}")
            return {"type": "discounts", "error": str(e)}
    
    def _enhance_with_specialized_data(self, result: Dict[str, Any], gid: str, specialized_data: Dict[str, Any]) -> Dict[str, Any]:
        """Enhance location data with specialized data"""
        try:
            location_name = result.get("location", "")
            
            # Add timezone information
            if "2046372794" in specialized_data:
                timezone_data = specialized_data["2046372794"]
                if timezone_data.get("type") == "time_zones":
                    timezone = timezone_data.get("locations", {}).get(location_name, "")
                    if timezone:
                        result["metadata"]["timezone"] = timezone
            
            # Add discount information
            if "885243828" in specialized_data:
                discount_data = specialized_data["885243828"]
                if discount_data.get("type") == "discounts":
                    discount = discount_data.get("locations", {}).get(location_name, "")
                    if discount:
                        result["operational_rules"]["discounts"] = discount
            
            # Add truck/storage information
            if "895613602" in specialized_data:
                trucks_data = specialized_data["895613602"]
                if trucks_data.get("type") == "trucks_storage_cx":
                    location_info = trucks_data.get("locations", {}).get(location_name, {})
                    if location_info:
                        result["metadata"]["truck_count"] = location_info.get("trucks", "")
                        result["metadata"]["storage_info"] = location_info.get("storage", "")
                        result["metadata"]["cx_care"] = location_info.get("cx_care", "")
                        result["metadata"]["contact"] = location_info.get("contact", "")
                        result["metadata"]["direct_line"] = location_info.get("direct_line", "")
            
            return result
            
        except Exception as e:
            logger.error(f"‚ùå Error enhancing data with specialized data for GID {gid}: {e}")
            return result
    
    def _fallback_parse_csv(self, csv_text: str, gid: str) -> Dict[str, Any]:
        """Fallback parsing method if smart parser fails"""
        try:
            # Basic CSV parsing
            rows = list(csv.reader(StringIO(csv_text)))
            
            # Extract basic location info
            location_details = {}
            for row in rows[:20]:  # Check first 20 rows
                row_text = ' '.join([cell for cell in row if cell])
                
                # Extract OPS MANAGER
                if 'OPS MANAGER:' in row_text:
                    ops_match = re.search(r'OPS MANAGER:\s*([^,]+)', row_text)
                    if ops_match:
                        location_details['ops_manager'] = ops_match.group(1).strip()
                
                # Extract ADDRESS
                if 'ADDRESS:' in row_text:
                    address_match = re.search(r'ADDRESS:\s*([^"]+)(?:[^,]*?)(?:,|$)', row_text)
                    if address_match:
                        address = address_match.group(1).strip()
                        if '1 HOUR MINIMUMS' in address:
                            address = address.split('1 HOUR MINIMUMS')[0].strip()
                        location_details['address'] = address
            
            # Basic result structure
            result = {
                "location": f"GID_{gid}",
                "calendar_hourly_price": {},
                "metadata": {
                    "ops_manager": location_details.get("ops_manager", ""),
                    "address": location_details.get("address", ""),
                    "email": "",
                    "terminal_id": "",
                    "intersection": "",
                    "truck_count": "",
                    "sales_phone": "",
                    "timezone": ""
                },
                "pricing_formula": {
                    "description": "Basic pricing formula",
                    "formulas": {
                        "1_truck": "base_price + 60 * (movers - 1)",
                        "2_trucks": "2 * (base_price + 60 * (movers - 1))",
                        "max_movers_per_truck": 3
                    },
                    "crew_rates": {},
                    "important_notes": ""
                },
                "operational_rules": {},
                "lat": 0.0,
                "lng": 0.0,
                "address": location_details.get("address", ""),
                "filename": f"gid_{gid}.json"
            }
            
            logger.warning(f"‚ö†Ô∏è Using fallback parser for GID {gid}")
            return result
            
        except Exception as e:
            logger.error(f"‚ùå Fallback parser also failed for GID {gid}: {e}")
            return {
                "location": f"GID_{gid}",
                "calendar_hourly_price": {},
                "metadata": {},
                "pricing_formula": {},
                "operational_rules": {},
                "lat": 0.0,
                "lng": 0.0,
                "address": "",
                "filename": f"gid_{gid}.json"
            }
    
    def _extract_location_details_from_csv(self, rows: List[List[str]], gid: str = None) -> Dict[str, Any]:
        """Extract location details from CSV rows, handling different CSV structures"""
        import re
        location_details = {}
        phone_pattern = re.compile(r'(\d{3}[-.\s]?\d{3}[-.\s]?\d{4})')
        
        # Use GID mapping to get location name if available
        if gid and gid in self.gid_location_mapping:
            location_details['name'] = self.gid_location_mapping[gid]
        
        # Check if this is a calendar-only CSV (no location details)
        has_location_details = False
        for row in rows[:10]:  # Check first 10 rows
            for cell in row:
                if cell and any(keyword in cell.upper() for keyword in ['LOCATION DETAILS', 'ADDRESS:', 'OPS MANAGER', 'E-TRANSFER']):
                    has_location_details = True
                    break
            if has_location_details:
                break
        
        if not has_location_details:
            # This is likely a calendar-only CSV, return minimal details
            if gid and gid in self.gid_location_mapping:
                return {
                    'name': self.gid_location_mapping[gid],
                    'address': '',
                    'sales_phone': '',
                    'email': '',
                    'truck_count': ''
                }
            else:
                return {
                    'name': f"Location {gid}" if gid else "Unknown Location",
                    'address': '',
                    'sales_phone': '',
                    'email': '',
                    'truck_count': ''
                }
        
        # Parse location details from CSV
        for i, row in enumerate(rows):
            if i > 30:  # Check more rows for location details
                break
            for cell in row:
                if not cell:
                    continue
                    
                cell_upper = cell.upper()
                
                # Extract address
                if 'ADDRESS:' in cell_upper:
                    location_details['address'] = cell.replace('ADDRESS:', '').strip().strip('"')
                
                # Extract location name from LOCATION DETAILS
                elif 'LOCATION DETAILS:' in cell_upper:
                    location_text = cell.replace('LOCATION DETAILS:', '').strip()
                    if location_text and location_text.upper() not in ['LOCATION DETAILS', '']:
                        location_details['name'] = location_text
                
                # Extract phone from OPS MANAGER
                elif 'OPS MANAGER:' in cell_upper:
                    phone_match = phone_pattern.search(cell)
                    if phone_match:
                        location_details['sales_phone'] = phone_match.group(1)
                
                # Extract sales phone
                elif 'SALES #:' in cell_upper:
                    location_details['sales_phone'] = cell.replace('SALES #:', '').strip()
                
                # Extract email
                elif 'E-TRANSFER:' in cell_upper:
                    location_details['email'] = cell.replace('E-TRANSFER:', '').strip()
                
                # Extract truck count
                elif '# OF TRUCKS:' in cell_upper:
                    location_details['truck_count'] = cell.replace('# OF TRUCKS:', '').strip()
                
                # Extract owner and phone
                elif 'OWNER:' in cell_upper:
                    owner_match = re.search(r'Owner:\s*([^|\n]+)', cell, re.IGNORECASE)
                    if owner_match:
                        location_details['owner'] = owner_match.group(1).strip()
                    phone_match = phone_pattern.search(cell)
                    if phone_match:
                        location_details['phone'] = phone_match.group(1)
                
                # Fallback: extract any phone number if not already found
                if 'phone' not in location_details and 'sales_phone' not in location_details:
                    phone_match = phone_pattern.search(cell)
                    if phone_match:
                        location_details['phone'] = phone_match.group(1)
        
        # Extract location name from address if not found elsewhere
        if 'name' not in location_details and 'address' in location_details:
            address = location_details['address']
            # Try to extract city name from address
            # Pattern: "Street Address, City, Province" or "City, Province"
            parts = address.split(',')
            if len(parts) >= 2:
                # Get the city part (usually second to last)
                city = parts[-2].strip() if len(parts) > 2 else parts[-1].strip()
                # Clean up common prefixes
                city = re.sub(r'^ADDRESS:\s*', '', city, flags=re.IGNORECASE)
                if city and len(city) > 2 and not city.isdigit():
                    location_details['name'] = city.upper()
        
        # Final fallback: use GID mapping or create generic name
        if 'name' not in location_details:
            if gid and gid in self.gid_location_mapping:
                location_details['name'] = self.gid_location_mapping[gid]
            else:
                location_details['name'] = f"Location {gid}" if gid else "Unknown Location"
        
        return location_details
    
    def _extract_pricing_tables_from_csv(self, rows: List[List[str]]) -> Dict[str, Any]:
        """Extract pricing tables from CSV rows"""
        pricing_tables = {}
        
        # Look for crew/truck pricing patterns throughout the file
        for i, row in enumerate(rows):
            for cell in row:
                if cell and '1 Truck' in cell:
                    # Found the crew/truck pricing matrix header
                    pricing_tables['crew_truck_matrix'] = self._parse_crew_truck_pricing(rows, i)
                    break
            if 'crew_truck_matrix' in pricing_tables:
                break
        
        return pricing_tables
    
    def _parse_crew_truck_pricing(self, rows: List[List[str]], start_row: int) -> Dict[str, Any]:
        """Parse crew/truck pricing matrix"""
        pricing = {}
        crew_rates = {}
        
        # Look for pricing patterns like "119 > 179 > 259" in the next 15 rows
        for i in range(start_row, min(start_row + 15, len(rows))):
            row = rows[i]
            for cell in row:
                if cell and '>' in cell and any(char.isdigit() for char in cell):
                    # Parse pricing like "119 > 179 > 259"
                    prices = [p.strip() for p in cell.split('>')]
                    if len(prices) >= 2:
                        try:
                            crew_2 = float(prices[0])
                            crew_3 = float(prices[1]) if len(prices) > 1 else crew_2
                            crew_4 = float(prices[2]) if len(prices) > 2 else crew_3
                            
                            # Store the first valid pricing we find
                            if not crew_rates:
                                crew_rates = {
                                    '2_crew': crew_2,
                                    '3_crew': crew_3,
                                    '4_crew': crew_4
                                }
                        except ValueError:
                            continue
        
        if crew_rates:
            pricing['crew_rates'] = crew_rates
        
        # Add fuel charges placeholder (calculated by vendor engine based on travel time)
        pricing['fuel_charges'] = {
            'base_rate_per_hour': 25.0,  # Base fuel charge per hour of travel
            'calculation_method': 'travel_time_based'
        }
        
        return pricing
    
    def _extract_calendar_data_from_csv(self, rows: List[List[str]], gid: str = None) -> Dict[str, Any]:
        """Extract calendar data (daily rates) from CSV rows using specialized parsers"""
        calendar_data = {'daily_rates': {}}
        
        # Always log that we're in this method
        logger.info(f"üîç _extract_calendar_data_from_csv called for GID: {gid} with {len(rows)} rows")
        
        # First, try specialized parser for this GID
        if gid and is_gid_supported(gid):
            logger.info(f"üîç Using specialized parser for GID: {gid}")
            try:
                result = parse_gid_specialized(gid, rows)
                if result and result.get('calendar_data', {}).get('daily_rates'):
                    logger.info(f"‚úÖ Specialized parser for GID {gid} found {len(result['calendar_data']['daily_rates'])} daily rates")
                    return result['calendar_data']
            except Exception as e:
                logger.error(f"‚ùå Error in specialized parser for GID {gid}: {e}")
        
        # Fallback to original parsing methods if specialized parser fails
        
        # Log the total number of rows for debugging
        logger.info(f"üìä Processing CSV with {len(rows)} rows")
        
        # Log some sample content to understand the structure
        sample_rows = []
        for i in range(min(10, len(rows))):
            sample_cells = [cell[:20] for cell in rows[i][:5] if cell]  # First 5 cells, first 20 chars
            if sample_cells:
                sample_rows.append(f"Row {i}: {sample_cells}")
        
        logger.info(f"üìã Sample rows: {sample_rows[:3]}")  # Log first 3 sample rows
        
        # Method 1: Look for month headers and daily rates
        month_headers_found = 0
        for i, row in enumerate(rows):
            if i > 300:  # Check more rows to find calendar data
                break
                
            for j, cell in enumerate(row):
                if cell and any(month in cell.upper() for month in ['JAN', 'FEB', 'MAR', 'APR', 'MAY', 'JUN', 'JUL', 'AUG', 'SEP', 'OCT', 'NOV', 'DEC']):
                    # Found a month header - extract month number
                    month_num = self._extract_month_number(cell)
                    if month_num:
                        month_headers_found += 1
                        logger.info(f"üîç Method 1: Found month header '{cell}' at row {i}, col {j}")
                        month_rates = self._parse_month_calendar_from_csv(rows, i, j, month_num)
                        calendar_data['daily_rates'].update(month_rates)
        
        logger.info(f"üìÖ Method 1: Found {month_headers_found} month headers")
        
        # Method 2: Look for year headers (like "2025") and parse calendars below them
        year_headers_found = 0
        for i, row in enumerate(rows):
            if i > 300:  # Check more rows to find calendar data
                break
                
            for j, cell in enumerate(row):
                if cell and '2025' in cell:
                    # Found year header - look for calendars below
                    year_headers_found += 1
                    logger.info(f"üîç Method 2: Found year header '{cell}' at row {i}, col {j}")
                    year_calendars = self._parse_year_calendar_from_csv(rows, i, j)
                    calendar_data['daily_rates'].update(year_calendars)
        
        logger.info(f"üìÖ Method 2: Found {year_headers_found} year headers")
        
        # Method 3: Look for location-specific calendar headers (e.g., "BARRIE 1 HR MIN STARTING AT 179/HR")
        location_headers_found = 0
        for i, row in enumerate(rows):
            if i > 300:  # Check more rows to find calendar data
                break
                
            for j, cell in enumerate(row):
                if cell and ('HR MIN STARTING AT' in cell or '1 HR MIN' in cell):
                    # Found location-specific calendar header - parse calendar below
                    location_headers_found += 1
                    logger.info(f"üîç Method 3: Found location header '{cell}' at row {i}, col {j}")
                    location_calendars = self._parse_location_calendar_from_csv(rows, i, j)
                    calendar_data['daily_rates'].update(location_calendars)
        
        logger.info(f"üìÖ Method 3: Found {location_headers_found} location headers")
        
        # Method 4: Look for any row with day numbers (1-31) and rates
        if not calendar_data['daily_rates']:
            logger.info(f"üîç Method 4: Trying generic calendar structure parsing")
            calendar_data['daily_rates'] = self._parse_any_calendar_structure(rows)
        
        # Method 5: Look for day-of-week calendar format (SUNDAY, MONDAY, etc.)
        if not calendar_data['daily_rates']:
            logger.info(f"üîç Method 5: Trying day-of-week calendar format")
            try:
                calendar_data['daily_rates'] = self._parse_day_of_week_calendar(rows)
            except Exception as e:
                logger.error(f"Error in Method 5: {e}")
                calendar_data['daily_rates'] = {}
        
        # Method 6: Always try day-of-week format for debugging
        logger.info(f"üîç Method 6: Always trying day-of-week calendar format")
        try:
            day_of_week_rates = self._parse_day_of_week_calendar(rows)
            logger.info(f"üîç Method 6 returned: {len(day_of_week_rates)} rates")
            if day_of_week_rates:
                logger.info(f"‚úÖ Method 6 found {len(day_of_week_rates)} rates")
                calendar_data['daily_rates'].update(day_of_week_rates)
        except Exception as e:
            logger.error(f"Error in Method 6: {e}")
            import traceback
            logger.error(f"Traceback: {traceback.format_exc()}")
        
        # Method 7: Force test with BRANTFORD GID
        if gid == "1324028052":
            logger.info(f"üîç Method 7: Testing BRANTFORD GID specifically")
            try:
                day_of_week_rates = self._parse_day_of_week_calendar(rows)
                logger.info(f"üîç Method 7 for BRANTFORD returned: {len(day_of_week_rates)} rates")
                if day_of_week_rates:
                    logger.info(f"‚úÖ Method 7 for BRANTFORD found {len(day_of_week_rates)} rates")
                    calendar_data['daily_rates'].update(day_of_week_rates)
            except Exception as e:
                logger.error(f"Error in Method 7 for BRANTFORD: {e}")
                import traceback
                logger.error(f"Traceback: {traceback.format_exc()}")
        
        # Method 8: BRANTFORD-specific pattern matching
        if gid == "1324028052":
            logger.info(f"üîç Method 8: BRANTFORD-specific pattern matching")
            try:
                # Convert rows to text
                csv_text = '\n'.join([','.join(row) for row in rows])
                
                # Look for the specific BRANTFORD pattern: 
                # Line with: ,,,1,2,3,4 (day numbers with empty cells)
                # Line with: ,,,159,159,159,139 (rates with empty cells)
                
                # Pattern: empty cells, then day numbers, then empty cells
                day_pattern = r',*,(\d{1,2}),(\d{1,2}),(\d{1,2}),(\d{1,2}),*'
                # Pattern: empty cells, then rates, then empty cells  
                rate_pattern = r',*,(\d{2,3}),(\d{2,3}),(\d{2,3}),(\d{2,3}),*'
                
                day_matches = list(re.finditer(day_pattern, csv_text))
                rate_matches = list(re.finditer(rate_pattern, csv_text))
                
                logger.info(f"üîç Found {len(day_matches)} day patterns and {len(rate_matches)} rate patterns")
                
                # Match days with rates (they should be in pairs)
                for i in range(min(len(day_matches), len(rate_matches))):
                    day_match = day_matches[i]
                    rate_match = rate_matches[i]
                    
                    days = [int(day_match.group(j)) for j in range(1, 5)]
                    rates = [float(rate_match.group(j)) for j in range(1, 5)]
                    
                    # Validate: days should be 1-31, rates should be 50-500
                    if all(1 <= day <= 31 for day in days) and all(50 <= rate <= 500 for rate in rates):
                        logger.info(f"üîç Method 8 found valid pattern: days={days}, rates={rates}")
                        
                        # Determine month from context
                        month_num = self._find_month_from_context(rows, 0)
                        
                        for day_num, rate in zip(days, rates):
                            date_key = f"{month_num}-{day_num:02d}"
                            calendar_data['daily_rates'][date_key] = rate
                            logger.info(f"‚úÖ Method 8 parsed rate for {date_key}: ${rate}")
                        
            except Exception as e:
                logger.error(f"Error in Method 8: {e}")
                import traceback
                logger.error(f"Traceback: {traceback.format_exc()}")
        
        # Method 9: Force BRANTFORD to use regex parsing
        if gid == "1324028052":
            logger.info(f"üîç Method 9: Force BRANTFORD regex parsing")
            try:
                # Convert rows to text
                csv_text = '\n'.join([','.join(row) for row in rows])
                
                # Simple pattern: find day numbers followed by rates
                # Look for: 1,2,3,4 followed by 159,159,159,139
                pattern = r'(\d{1,2}),(\d{1,2}),(\d{1,2}),(\d{1,2})\s*\n\s*(\d{2,3}),(\d{2,3}),(\d{2,3}),(\d{2,3})'
                matches = list(re.finditer(pattern, csv_text))
                
                logger.info(f"üîç Method 9 found {len(matches)} exact pattern matches")
                
                for i, match in enumerate(matches[:10]):  # Limit to first 10 matches
                    days = [int(match.group(j)) for j in range(1, 5)]
                    rates = [float(match.group(j)) for j in range(5, 9)]
                    
                    logger.info(f"üîç Method 9 match {i+1}: days={days}, rates={rates}")
                    
                    # Determine month from context
                    month_num = self._find_month_from_context(rows, 0)
                    
                    for day_num, rate in zip(days, rates):
                        date_key = f"{month_num}-{day_num:02d}"
                        calendar_data['daily_rates'][date_key] = rate
                        logger.info(f"‚úÖ Method 9 parsed rate for {date_key}: ${rate}")
                        
            except Exception as e:
                logger.error(f"Error in Method 9: {e}")
                import traceback
                logger.error(f"Traceback: {traceback.format_exc()}")
        
        # Method 10: Smart Multi-Location Regex Parsing
        logger.info(f"üîç Method 10: Smart Multi-Location Regex Parsing for GID {gid}")
        try:
            # Convert rows to text
            csv_text = '\n'.join([','.join(row) for row in rows])
            
            # 1. Extract Terminal ID and Location Details
            terminal_id_pattern = r'Terminal ID:\s*([A-Za-z0-9]+)'
            terminal_matches = re.finditer(terminal_id_pattern, csv_text)
            for match in terminal_matches:
                terminal_id = match.group(1)
                logger.info(f"üîç Found Terminal ID: {terminal_id}")
            
            # 2. Extract Location Names (like ABBOTSFORD, DURHAM, AJAX)
            location_pattern = r'([A-Z]{3,})\s*,,,.*?1 HR MIN STARTING AT|([A-Z]{3,})\s*,,,,,,,([A-Z]{3})'
            location_matches = re.finditer(location_pattern, csv_text)
            locations_found = []
            for match in location_matches:
                location = match.group(1) or match.group(2)
                if location and location not in locations_found:
                    locations_found.append(location)
                    logger.info(f"üîç Found Location: {location}")
            
            # 3. Smart Calendar Pattern Matching
            # Pattern 1: Standard day numbers followed by rates
            standard_pattern = r'(\d{1,2}),(\d{1,2}),(\d{1,2}),(\d{1,2})\s*\n\s*(\d{2,3}),(\d{2,3}),(\d{2,3}),(\d{2,3})'
            standard_matches = list(re.finditer(standard_pattern, csv_text))
            
            # Pattern 2: Split rates like "159/149"
            split_rate_pattern = r'(\d{1,2}),(\d{1,2}),(\d{1,2}),(\d{1,2})\s*\n\s*(\d{2,3}/\d{2,3}),(\d{2,3}/\d{2,3}),(\d{2,3}/\d{2,3}),(\d{2,3}/\d{2,3})'
            split_matches = list(re.finditer(split_rate_pattern, csv_text))
            
            # Pattern 3: Mixed rates (some standard, some split)
            mixed_pattern = r'(\d{1,2}),(\d{1,2}),(\d{1,2}),(\d{1,2})\s*\n\s*((?:\d{2,3}(?:/\d{2,3})?),){3}(?:\d{2,3}(?:/\d{2,3})?)'
            mixed_matches = list(re.finditer(mixed_pattern, csv_text))
            
            logger.info(f"üîç Method 10 found {len(standard_matches)} standard patterns, {len(split_matches)} split patterns, {len(mixed_matches)} mixed patterns")
            
            # Process standard patterns
            for i, match in enumerate(standard_matches[:20]):  # Limit to first 20
                days = [int(match.group(j)) for j in range(1, 5)]
                rates = [float(match.group(j)) for j in range(5, 9)]
                
                logger.info(f"üîç Method 10 standard match {i+1}: days={days}, rates={rates}")
                
                # Determine month from context
                month_num = self._find_month_from_context(rows, 0)
                
                for day_num, rate in zip(days, rates):
                    date_key = f"{month_num}-{day_num:02d}"
                    calendar_data['daily_rates'][date_key] = rate
                    logger.info(f"‚úÖ Method 10 parsed standard rate for {date_key}: ${rate}")
            
            # Process split rate patterns
            for i, match in enumerate(split_matches[:10]):  # Limit to first 10
                days = [int(match.group(j)) for j in range(1, 5)]
                split_rates = [match.group(j) for j in range(5, 9)]
                
                logger.info(f"üîç Method 10 split match {i+1}: days={days}, split_rates={split_rates}")
                
                # Determine month from context
                month_num = self._find_month_from_context(rows, 0)
                
                for day_num, split_rate in zip(days, split_rates):
                    # Handle split rates like "159/149" - use the first rate
                    if '/' in split_rate:
                        rate = float(split_rate.split('/')[0])
                    else:
                        rate = float(split_rate)
                    
                    date_key = f"{month_num}-{day_num:02d}"
                    calendar_data['daily_rates'][date_key] = rate
                    logger.info(f"‚úÖ Method 10 parsed split rate for {date_key}: ${rate}")
                        
        except Exception as e:
            logger.error(f"Error in Method 10: {e}")
            import traceback
            logger.error(f"Traceback: {traceback.format_exc()}")
        
        # Log calendar parsing results
        daily_rates_count = len(calendar_data.get('daily_rates', {}))
        if daily_rates_count > 0:
            logger.info(f"‚úÖ Successfully parsed {daily_rates_count} daily rates from calendar data")
            # Log first few rates for debugging
            first_rates = list(calendar_data['daily_rates'].items())[:5]
            logger.info(f"üìÖ Sample rates: {first_rates}")
        else:
            logger.warning(f"‚ö†Ô∏è No daily rates found in calendar data")
        
        return calendar_data
    
    def _extract_month_number(self, month_text: str) -> Optional[str]:
        """Extract month number from month text"""
        month_mapping = {
            'JAN': '01', 'FEB': '02', 'MAR': '03', 'APR': '04',
            'MAY': '05', 'JUN': '06', 'JUL': '07', 'AUG': '08',
            'SEP': '09', 'OCT': '10', 'NOV': '11', 'DEC': '12'
        }
        
        for month_name, month_num in month_mapping.items():
            if month_name in month_text.upper():
                return month_num
        return None

    def _parse_month_calendar_from_csv(self, rows: List[List[str]], start_row: int, start_col: int, month_num: str) -> Dict[str, float]:
        """Parse daily rates for a specific month from CSV - use MM-DD format"""
        daily_rates = {}
        
        # Initialize variables
        day_row = None
        rate_row = None
        
        # Look for calendar structure in the next 20 rows
        for i in range(start_row + 1, min(start_row + 20, len(rows))):
            row = rows[i]
            if len(row) > start_col + 6:  # Need at least 7 columns for a week
                # Check if this row contains day numbers (1-31)
                days = []
                for j in range(start_col, start_col + 7):
                    cell = row[j] if j < len(row) else ""
                    if cell and cell.strip().isdigit():
                        day_num = int(cell.strip())
                        if 1 <= day_num <= 31:  # Valid day number
                            days.append(day_num)
                        else:
                            days.append(None)
                    else:
                        days.append(None)
                
                if any(days):  # Found day numbers
                    day_row = i
                    # Look for rate row (next row or two rows down)
                    for k in range(i + 1, min(i + 4, len(rows))):
                        rate_candidates = rows[k]
                        if len(rate_candidates) > start_col + 6:
                            # Check if this row contains numbers (rates)
                            rates = []
                            for j in range(start_col, start_col + 7):
                                cell = rate_candidates[j] if j < len(rate_candidates) else ""
                                if cell:
                                    # Extract first number from cell (hourly rates)
                                    numbers = re.findall(r'\d+', cell)
                                    if numbers:
                                        try:
                                            rate = float(numbers[0])
                                            # Only accept reasonable hourly rates (50-500)
                                            if 50 <= rate <= 500:
                                                rates.append(rate)
                                            else:
                                                rates.append(None)
                                        except ValueError:
                                            rates.append(None)
                                    else:
                                        rates.append(None)
                                else:
                                    rates.append(None)
                            
                            if any(r is not None for r in rates):
                                rate_row = k
                                break
                    break
        
        # Combine days and rates
        if day_row is not None and rate_row is not None:
            day_data = rows[day_row]
            rate_data = rows[rate_row]
            
            for j in range(start_col, start_col + 7):
                if j < len(day_data) and j < len(rate_data):
                    day_cell = day_data[j]
                    rate_cell = rate_data[j]
                    
                    if day_cell and day_cell.strip().isdigit():
                        day = int(day_cell.strip())
                        if 1 <= day <= 31:  # Valid day number
                            if rate_cell:
                                # Extract first number from rate cell
                                numbers = re.findall(r'\d+', rate_cell)
                                if numbers:
                                    try:
                                        rate = float(numbers[0])
                                        # Only accept reasonable hourly rates
                                        if 50 <= rate <= 500:
                                            # Use MM-DD format (no year) - this is what's important
                                            date_key = f"{month_num}-{day:02d}"
                                            daily_rates[date_key] = rate
                                            logger.info(f"Parsed rate for {date_key}: ${rate}")
                                    except ValueError:
                                        continue
        
        return daily_rates
    
    def _parse_year_calendar_from_csv(self, rows: List[List[str]], start_row: int, start_col: int) -> Dict[str, float]:
        """Parse calendar data below a year header (like '2025')"""
        daily_rates = {}
        
        # Look for month headers and calendars below the year header
        for i in range(start_row + 1, min(start_row + 50, len(rows))):
            row = rows[i]
            for j, cell in enumerate(row):
                if cell and any(month in cell.upper() for month in ['JAN', 'FEB', 'MAR', 'APR', 'MAY', 'JUN', 'JUL', 'AUG', 'SEP', 'OCT', 'NOV', 'DEC']):
                    # Found a month header - extract month number
                    month_num = self._extract_month_number(cell)
                    if month_num:
                        month_rates = self._parse_month_calendar_from_csv(rows, i, j, month_num)
                        daily_rates.update(month_rates)
        
        return daily_rates
    
    def _parse_location_calendar_from_csv(self, rows: List[List[str]], start_row: int, start_col: int) -> Dict[str, float]:
        """Parse calendar data below a location-specific header (e.g., 'BARRIE 1 HR MIN STARTING AT 179/HR')"""
        daily_rates = {}
        
        # Look for month headers and calendars below the location header
        for i in range(start_row + 1, min(start_row + 50, len(rows))):
            row = rows[i]
            for j, cell in enumerate(row):
                if cell and any(month in cell.upper() for month in ['JAN', 'FEB', 'MAR', 'APR', 'MAY', 'JUN', 'JUL', 'AUG', 'SEP', 'OCT', 'NOV', 'DEC']):
                    # Found a month header - extract month number
                    month_num = self._extract_month_number(cell)
                    if month_num:
                        month_rates = self._parse_month_calendar_from_csv(rows, i, j, month_num)
                        daily_rates.update(month_rates)
        
        return daily_rates
    
    def _parse_any_calendar_structure(self, rows: List[List[str]]) -> Dict[str, float]:
        """Parse any calendar structure by looking for day numbers and rates"""
        daily_rates = {}
        
        # Look for any row that contains day numbers (1-31)
        for i, row in enumerate(rows):
            if i > 400:  # Check more rows
                break
                
            # Check if this row contains day numbers
            day_numbers = []
            for j, cell in enumerate(row):
                if cell and cell.strip().isdigit():
                    day_num = int(cell.strip())
                    if 1 <= day_num <= 31:
                        day_numbers.append((j, day_num))
            
            if len(day_numbers) >= 3:  # Need at least 3 days to be a calendar
                # Found day numbers, look for rates in nearby rows
                for k in range(max(0, i-2), min(len(rows), i+3)):
                    rate_row = rows[k]
                    rates_found = []
                    
                    for col, day_num in day_numbers:
                        if col < len(rate_row):
                            cell = rate_row[col]
                            if cell:
                                # Extract first number from cell
                                numbers = re.findall(r'\d+', cell)
                                if numbers:
                                    try:
                                        rate = float(numbers[0])
                                        # Only accept reasonable hourly rates (50-500)
                                        if 50 <= rate <= 500:
                                            rates_found.append((day_num, rate))
                                    except ValueError:
                                        continue
                    
                    if len(rates_found) >= 2:  # Need at least 2 rates to be valid
                        # Determine month from context (try to find month info)
                        month_num = self._find_month_from_context(rows, i)
                        if month_num:
                            for day_num, rate in rates_found:
                                date_key = f"{month_num}-{day_num:02d}"
                                daily_rates[date_key] = rate
                                logger.info(f"Found rate for {date_key}: ${rate}")
                        break
        
        return daily_rates
    
    def _find_month_from_context(self, rows: List[List[str]], day_row: int) -> Optional[str]:
        """Find month number from context around the day row"""
        # Look for month info in nearby rows
        for i in range(max(0, day_row-10), min(len(rows), day_row+10)):
            row = rows[i]
            for cell in row:
                if cell:
                    month_num = self._extract_month_number(cell)
                    if month_num:
                        return month_num
        
        # If no month found, assume current month or next month
        from datetime import datetime
        current_month = datetime.now().month
        return f"{current_month:02d}"
    
    def _parse_day_of_week_calendar(self, rows: List[List[str]]) -> Dict[str, float]:
        """Parse calendar data in day-of-week format using smart regex-based approach"""
        daily_rates = {}
        
        # Convert all rows to a single string for regex searching
        csv_text = '\n'.join([','.join(row) for row in rows])
        
        # Pattern 1: Look for day-of-week headers followed by day numbers and rates
        # Pattern: SUNDAY,MONDAY,TUESDAY,WEDNESDAY,THURSDAY,FRIDAY,SATURDAY
        # Followed by: day numbers (1-31) and rates (50-500)
        
        # Find all day-of-week header patterns
        day_pattern = r'(SUNDAY|MONDAY|TUESDAY|WEDNESDAY|THURSDAY|FRIDAY|SATURDAY)'
        day_headers = re.findall(day_pattern, csv_text, re.IGNORECASE)
        
        if len(day_headers) >= 5:  # Need at least 5 days
            logger.info(f"üîç Found {len(day_headers)} day-of-week headers: {day_headers[:7]}")
            
            # Look for patterns like: day,day,day,day,day,day,day followed by numbers
            # This matches the calendar structure where days are in columns
            
            # Pattern for finding day numbers followed by rates
            # Look for: 1,2,3,4,5,6,7 followed by 159,159,159,139,139,139,199
            calendar_pattern = r'((?:\d{1,2},?){3,7})\s*\n\s*((?:\d{2,3},?){3,7})'
            
            matches = re.finditer(calendar_pattern, csv_text)
            
            for match in matches:
                day_numbers_str = match.group(1)
                rates_str = match.group(2)
                
                # Parse day numbers
                day_numbers = []
                for day_str in day_numbers_str.split(','):
                    day_str = day_str.strip()
                    if day_str.isdigit():
                        day_num = int(day_str)
                        if 1 <= day_num <= 31:
                            day_numbers.append(day_num)
                
                # Parse rates
                rates = []
                for rate_str in rates_str.split(','):
                    rate_str = rate_str.strip()
                    # Extract first number from rate string (handle cases like "219/169")
                    numbers = re.findall(r'\d+', rate_str)
                    if numbers:
                        try:
                            rate = float(numbers[0])
                            if 50 <= rate <= 500:
                                rates.append(rate)
                            else:
                                rates.append(None)
                        except ValueError:
                            rates.append(None)
                    else:
                        rates.append(None)
                
                # Match days with rates
                if len(day_numbers) >= 3 and len(rates) >= 3:
                    logger.info(f"üîç Found calendar pattern: days={day_numbers}, rates={rates}")
                    
                    # Determine month from context
                    month_num = self._find_month_from_context(rows, 0)  # Use first row as reference
                    
                    for i, (day_num, rate) in enumerate(zip(day_numbers, rates)):
                        if rate is not None:
                            date_key = f"{month_num}-{day_num:02d}"
                            daily_rates[date_key] = rate
                            logger.info(f"‚úÖ Parsed rate for {date_key}: ${rate}")
        
        # Pattern 2: Look for specific rate patterns in the CSV
        # Find patterns like: 159,159,159,139 (hourly rates)
        rate_pattern = r'(\d{2,3}),(\d{2,3}),(\d{2,3}),(\d{2,3})'
        rate_matches = re.finditer(rate_pattern, csv_text)
        
        for match in rate_matches:
            rates = [float(match.group(i)) for i in range(1, 5)]
            # Check if these look like valid hourly rates
            if all(50 <= rate <= 500 for rate in rates):
                logger.info(f"üîç Found rate pattern: {rates}")
                
                # Try to find corresponding day numbers nearby
                # Look for day numbers in the same area
                start_pos = max(0, match.start() - 200)
                end_pos = min(len(csv_text), match.end() + 200)
                nearby_text = csv_text[start_pos:end_pos]
                
                # Look for day numbers near the rates
                day_pattern_nearby = r'(\d{1,2}),(\d{1,2}),(\d{1,2}),(\d{1,2})'
                day_matches = re.finditer(day_pattern_nearby, nearby_text)
                
                for day_match in day_matches:
                    days = [int(day_match.group(i)) for i in range(1, 5)]
                    if all(1 <= day <= 31 for day in days):
                        logger.info(f"üîç Found day pattern near rates: {days}")
                        
                        # Determine month from context
                        month_num = self._find_month_from_context(rows, 0)
                        
                        for day_num, rate in zip(days, rates):
                            date_key = f"{month_num}-{day_num:02d}"
                            daily_rates[date_key] = rate
                            logger.info(f"‚úÖ Parsed rate for {date_key}: ${rate}")
                        break
        
        return daily_rates
    
    def _extract_operational_notes_from_csv(self, rows: List[List[str]]) -> Dict[str, Any]:
        """Extract operational notes from CSV rows"""
        notes = {}
        
        for i, row in enumerate(rows):
            for cell in row:
                if cell and 'IMPORTANT NOTES' in cell:
                    # Found operational notes section
                    notes['operational_notes'] = cell.strip()
                    
                    # Look for additional notes in the same row or next few rows
                    additional_notes = []
                    for j in range(i, min(i + 5, len(rows))):
                        row_data = rows[j]
                        for cell_data in row_data:
                            if cell_data and len(cell_data) > 20 and 'IMPORTANT NOTES' not in cell_data:
                                additional_notes.append(cell_data.strip())
                    
                    if additional_notes:
                        notes['additional_notes'] = additional_notes
                    break
            if 'operational_notes' in notes:
                break
        
        return notes
    
    def get_dispatcher_data(self, location: str) -> Optional[Dict[str, Any]]:
        """Get data for a specific dispatcher location"""
        all_data = self.get_all_dispatchers_data()
        return all_data.get(location)
    
    def update_dispatcher_cache(self, location: str) -> bool:
        """Update cache for a specific dispatcher"""
        try:
            dispatcher_data = self.get_dispatcher_data(location)
            if dispatcher_data:
                # Here you would update the database cache
                # For now, just log the update
                logger.info(f"Updated cache for dispatcher: {location}")
                return True
            return False
        except Exception as e:
            logger.error(f"Error updating cache for {location}: {e}")
            return False
    
    def get_daily_rate(self, location: str, target_date: date) -> Optional[float]:
        """Get daily rate for a specific location and date, ignoring year (always forward from today)."""
        from datetime import timedelta
        dispatcher_data = self.get_dispatcher_data(location)
        if not dispatcher_data:
            return None
        daily_rates = dispatcher_data.get("calendar_data", {}).get("daily_rates", {})
        # Try to find the next available rate from the requested date forward
        for offset in range(0, 366):
            check_date = target_date + timedelta(days=offset)
            mmdd = check_date.strftime("%m-%d")
            if mmdd in daily_rates:
                return daily_rates[mmdd]
        return None
    
    def get_crew_rate(self, location: str, crew_size: int) -> Optional[float]:
        """Get crew rate for a specific location and crew size"""
        dispatcher_data = self.get_dispatcher_data(location)
        if not dispatcher_data:
            return None
        
        crew_rates = dispatcher_data.get("pricing_tables", {}).get("crew_rates", {})
        return crew_rates.get(str(crew_size))

    def fetch_public_tab_csv(self, spreadsheet_id: str, gid: str) -> list:
        """Fetch a public Google Sheet tab as CSV and return as list of rows"""
        url = f"https://docs.google.com/spreadsheets/d/{spreadsheet_id}/export?format=csv&gid={gid}"
        response = requests.get(url, allow_redirects=True, timeout=30)
        response.raise_for_status()
        csv_content = response.content.decode('utf-8')
        reader = csv.reader(StringIO(csv_content))
        return list(reader)

    def parse_and_normalize_public_tab(self, spreadsheet_id: str, gid: str) -> dict:
        """Fetch, parse, and normalize a public Google Sheet tab for a dispatcher/location, reading up to 10000 lines."""
        rows = self.fetch_public_tab_csv(spreadsheet_id, gid)[:10000]
        # Extract location details
        location_details = self._extract_location_details_from_csv(rows)
        # Extract operational notes
        operational_notes = self._extract_operational_notes_from_csv(rows)
        # Extract pricing tables (crew/truck matrix)
        pricing_tables = self._robust_extract_pricing_tables(rows)
        # Extract calendar data (daily/monthly rates)
        calendar_data = self._robust_extract_calendar_data(rows)
        # Get dispatcher/location name and address with improved extraction
        name = self._extract_best_location_name(location_details, gid)
        address = location_details.get('address', '')
        # Geocode address for coordinates
        coordinates = mapbox_service.get_coordinates(address) if address else None
        # Extract base_rate from calendar data (daily rates) for LGM
        base_rates = {}
        for vendor in ['lets-get-moving', 'easy2go', 'velocity-movers', 'pierre-sons']:
            base_rates[vendor] = None
            
        # For LGM, get base rate from calendar data
        if calendar_data and 'daily_rates' in calendar_data and calendar_data['daily_rates']:
            # Get today's rate or find a common rate
            from datetime import datetime
            today = datetime.now().strftime('%Y-%m-%d')
            
            if today in calendar_data['daily_rates']:
                base_rates['lets-get-moving'] = calendar_data['daily_rates'][today]
                print(f"LGM Base Rate: Using today's rate ${base_rates['lets-get-moving']}")
            else:
                # Find the most common rate
                rates = list(calendar_data['daily_rates'].values())
                if rates:
                    from collections import Counter
                    most_common_rate = Counter(rates).most_common(1)[0][0]
                    base_rates['lets-get-moving'] = most_common_rate
                    print(f"LGM Base Rate: Using most common rate ${most_common_rate}")
        
        # For other vendors, try to get from pricing_tables
        for vendor in ['easy2go', 'velocity-movers', 'pierre-sons']:
            try:
                if '2_trucks' in pricing_tables and 2 in pricing_tables['2_trucks']:
                    base_rates[vendor] = pricing_tables['2_trucks'][2]['base']
                elif '1_truck' in pricing_tables and 2 in pricing_tables['1_truck']:
                    base_rates[vendor] = pricing_tables['1_truck'][2]['base']
            except Exception:
                pass
        return {
            "location": name,
            "location_details": location_details,
            "address": address,
            "coordinates": coordinates,
            "base_rates": base_rates,
            "pricing_tables": pricing_tables,
            "calendar_data": calendar_data,
            "operational_notes": operational_notes,
            "last_updated": datetime.now().isoformat()
        }

    def _robust_extract_pricing_tables(self, rows: list) -> dict:
        """Robustly extract pricing tables (crew/truck matrix) from up to 500 rows."""
        pricing_data = {
            "1_truck": {},
            "2_trucks": {},
            "crew_rates": {},
            "formulas": {}
        }
        found_header = False
        crew_headers = []
        for i, row in enumerate(rows):
            if not row or len(row) < 2:
                continue
            row_text = " ".join(row).strip()
            # Look for truck pricing headers
            if ("1 Truck" in row_text or "2 Trucks" in row_text) and not found_header:
                found_header = True
                # Parse crew headers (e.g., '2 > 3 > 4')
                for cell in row:
                    if ">" in cell:
                        crew_headers = [int(s) for s in cell.replace('>', ' ').split() if s.isdigit()]
                continue
            if found_header:
                # Parse pricing rows (e.g., '119 > 179 > 259')
                for idx, cell in enumerate(row):
                    if ">" in cell:
                        prices = [float(p.strip()) for p in cell.split('>') if p.strip().replace('.', '', 1).isdigit()]
                        if crew_headers and len(prices) == len(crew_headers):
                            for j, crew_size in enumerate(crew_headers):
                                truck_key = "1_truck" if idx == 0 else "2_trucks"
                                pricing_data[truck_key][crew_size] = {
                                    "min": prices[j],
                                    "base": prices[j],
                                    "max": prices[j]
                                }
                # Stop if we hit a non-pricing row
                if not any(">" in cell for cell in row):
                    break
        return pricing_data

    def _robust_extract_calendar_data(self, rows: list) -> dict:
        """Extract calendar-based daily rates from LGM Google Sheets format (handles different CSV structures)."""
        import logging
        from datetime import date, timedelta
        import re
        
        calendar_data = {
            "daily_rates": {},
            "monthly_rates": {},
            "restricted_dates": []
        }
        
        # Find the current year (2025)
        current_year = 2025
        
        # Look for calendar data patterns
        month_names = ["JAN", "FEB", "MAR", "APR", "MAY", "JUN", "JUL", "AUG", "SEP", "OCT", "NOV", "DEC"]
        month_numbers = {"JAN": 1, "FEB": 2, "MAR": 3, "APR": 4, "MAY": 5, "JUN": 6,
                        "JUL": 7, "AUG": 8, "SEP": 9, "OCT": 10, "NOV": 11, "DEC": 12}
        
        # Check if this CSV has calendar data
        has_calendar_data = False
        for row in rows[:50]:  # Check first 50 rows
            for cell in row:
                if cell and any(month in cell.upper() for month in month_names):
                    has_calendar_data = True
                    break
            if has_calendar_data:
                break
        
        if not has_calendar_data:
            print("LGM Calendar: No calendar data found in this CSV")
            return calendar_data
        
        # Find calendar sections
        calendar_sections = []
        for row_idx, row in enumerate(rows):
            for col_idx, cell in enumerate(row):
                if cell and any(month in cell.upper() for month in month_names):
                    # Found a month header, look for the calendar grid
                    month_name = None
                    for month in month_names:
                        if month in cell.upper():
                            month_name = month
                            break
                    
                    if month_name:
                        # Look for the calendar grid starting from this row
                        calendar_sections.append({
                            'month': month_name,
                            'month_num': month_numbers[month_name],
                            'start_row': row_idx,
                            'year': current_year
                        })
        
        # Extract daily rates from calendar sections
        for section in calendar_sections:
            month_num = section['month_num']
            year = section['year']
            start_row = section['start_row']
            
            # Look for the calendar grid (usually starts 2-3 rows after month header)
            for row_idx in range(start_row + 2, min(start_row + 25, len(rows))):
                row = rows[row_idx]

                # Look for day numbers (1-31) and corresponding rates
                for col_idx, cell in enumerate(row):
                    if cell and cell.isdigit() and 1 <= int(cell) <= 31:
                        day = int(cell)

                        # Look for rate in the NEXT row (rates are below dates)
                        if row_idx + 1 < len(rows):
                            next_row = rows[row_idx + 1]
                            if col_idx < len(next_row) and next_row[col_idx]:
                                try:
                                    rate = int(next_row[col_idx])
                                    if 100 <= rate <= 300:  # Valid rate range
                                        date_key = f"{year}-{month_num:02d}-{day:02d}"
                                        calendar_data["daily_rates"][date_key] = rate
                                        print(f"LGM Calendar: {date_key} = ${rate}")
                                except (ValueError, IndexError):
                                    continue
        
        # If no daily rates found, try alternative parsing
        if not calendar_data["daily_rates"]:
            print("LGM Calendar: No daily rates found, trying alternative parsing...")
            # Look for any numeric values that could be rates
            for row_idx, row in enumerate(rows):
                for col_idx, cell in enumerate(row):
                    if cell and cell.isdigit() and 100 <= int(cell) <= 300:
                        # This might be a rate, try to find context
                        rate = int(cell)
                        # Look for date context in nearby cells
                        for check_row in range(max(0, row_idx-5), min(row_idx+5, len(rows))):
                            for check_col in range(max(0, col_idx-5), min(col_idx+5, len(rows[check_row]))):
                                check_cell = rows[check_row][check_col]
                                if check_cell and check_cell.isdigit() and 1 <= int(check_cell) <= 31:
                                    day = int(check_cell)
                                    # Try to find month context
                                    for month in month_names:
                                        if any(month in str(rows[i][j]).upper() for i in range(max(0, check_row-3), min(check_row+3, len(rows))) for j in range(max(0, check_col-3), min(check_col+3, len(rows[i])))):
                                            month_num = month_numbers[month]
                                            try:
                                                date_key = f"{current_year}-{month_num:02d}-{day:02d}"
                                                calendar_data["daily_rates"][date_key] = rate
                                                print(f"LGM Calendar (alt): {date_key} = ${rate}")
                                            except:
                                                continue
                                            break
                                    break
        
        # Final fallback: if still no rates, use a default rate
        if not calendar_data["daily_rates"]:
            print("LGM Calendar: Using fallback default rate $139")
            calendar_data["daily_rates"]["2025-03-01"] = 139
        
        print(f"LGM Calendar: Extracted {len(calendar_data['daily_rates'])} daily rates")
        return calendar_data

    def _extract_best_location_name(self, location_details: dict, gid: str = None) -> str:
        """Extract the best location name from various sources"""
        import re
        
        # Priority 1: Use GID mapping if available
        if gid and gid in self.gid_location_mapping:
            return self.gid_location_mapping[gid]
        
        # Priority 2: Use location name from details
        if 'name' in location_details and location_details['name']:
            name = location_details['name'].strip()
            # Clean up common prefixes
            name = re.sub(r'^LOCATION DETAILS:\s*', '', name)
            name = re.sub(r'^Owner:\s*', '', name)
            if name and name != 'GID_' + str(gid):
                return name
        
        # Priority 3: Extract from address
        if 'address' in location_details and location_details['address']:
            address = location_details['address']
            # Try to extract city name from address
            # Pattern: "City, Province" or "City, State"
            city_match = re.search(r'^([^,]+),\s*[A-Z]{2}', address)
            if city_match:
                city = city_match.group(1).strip()
                # Clean up common prefixes
                city = re.sub(r'^ADDRESS:\s*', '', city)
                if city:
                    return city
        
        # Priority 4: Use GID as fallback
        if gid:
            return f"Location_{gid}"
        
        return "Unknown Location"

    def list_public_tab_gids(self, spreadsheet_id: str) -> list:
        """List all public tab GIDs and names from the spreadsheet by scraping the HTML."""
        url = f"https://docs.google.com/spreadsheets/d/{spreadsheet_id}"
        response = requests.get(url, allow_redirects=True, timeout=30)
        response.raise_for_status()
        html = response.text
        # Find all gid and tab names in the HTML
        pattern = re.compile(r'"gid":(\d+),"title":"([^"]+)"')
        tabs = pattern.findall(html)
        return [(title, gid) for gid, title in tabs]

    def batch_normalize_public_tabs(self, spreadsheet_id: str, gids: list) -> dict:
        """Batch process all GIDs, returning a dict of normalized data for each location."""
        results = {}
        for gid in gids:
            try:
                results[gid] = self.parse_and_normalize_public_tab(spreadsheet_id, gid)
            except Exception as e:
                results[gid] = {"error": str(e)}
        return results

    def refresh_all_data(self):
        """Force refresh all Google Sheets data by clearing cache"""
        self._cache_data.clear()
        self._cache_timestamps.clear()
        logger.info("Google Sheets cache cleared - forcing refresh on next request")

    def force_cache_invalidation(self):
        """Force cache invalidation for next request"""
        self.refresh_all_data()
        logger.info("Google Sheets cache invalidation forced")

# Global instance
google_sheets_service = GoogleSheetsService() 